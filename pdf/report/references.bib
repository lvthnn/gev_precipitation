@book{coles_introduction_2001,
  address = {London},
  series = {Springer {Series} in {Statistics}},
  title = {An {Introduction} to {Statistical} {Modeling} of {Extreme} {Values}},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-84996-874-4 978-1-4471-3675-0},
  url = {http://link.springer.com/10.1007/978-1-4471-3675-0},
  urldate = {2025-01-16},
  publisher = {Springer},
  author = {Coles, Stuart},
  year = {2001},
  doi = {10.1007/978-1-4471-3675-0},
  keywords = {boundary element method, Mathematica, modeling, statistics,
              Statistics for life sciences, Statistics for the physical sciences},
}

@article{johannesson_approximate_2022,
  title = {Approximate {Bayesian} inference for analysis of spatiotemporal flood
           frequency data},
  volume = {16},
  issn = {1932-6157, 1941-7330},
  url = {
         https://projecteuclid.org/journals/annals-of-applied-statistics/volume-16/issue-2/Approximate-Bayesian-inference-for-analysis-of-spatiotemporal-flood-frequency-data/10.1214/21-AOAS1525.full
         },
  doi = {10.1214/21-AOAS1525},
  abstract = {Extreme floods cause casualties and widespread damage to property
              and vital civil infrastructure. Predictions of extreme floods,
              within gauged and ungauged catchments, is crucial to mitigate these
              disasters. In this paper a Bayesian framework is proposed for
              predicting extreme floods, using the generalized extreme-value
              (GEV) distribution. A major methodological challenge is to find a
              suitable parametrization for the GEV distribution when multiple
              covariates and/or latent spatial effects are involved and a time
              trend is present. Other challenges involve balancing model
              complexity and parsimony, using an appropriate model selection
              procedure and making inference based on a reliable and
              computationally efficient approach. We here propose a latent
              Gaussian modeling framework with a novel multivariate link function
              designed to separate the interpretation of the parameters at the
              latent level and to avoid unreasonable estimates of the shape and
              time trend parameters. Structured additive regression models, which
              include catchment descriptors as covariates and spatially
              correlated model components, are proposed for the four parameters
              at the latent level. To achieve computational efficiency with large
              datasets and richly parametrized models, we exploit a highly
              accurate and fast approximate Bayesian inference approach which can
              also be used to efficiently select models separately for each of
              the four regression models at the latent level. We applied our
              proposed methodology to annual peak river flow data from 554
              catchments across the United Kingdom. The framework performed well
              in terms of flood predictions for both ungauged catchments and
              future observations at gauged catchments. The results show that the
              spatial model components for the transformed location and scale
              parameters as well as the time trend are all important, and none of
              these should be ignored. Posterior estimates of the time trend
              parameters correspond to an average increase of about 1.5\% per
              decade with range 0.1\% to 2.8\% and reveal a spatial structure
              across the United Kingdom. When the interest lies in estimating
              return levels for spatial aggregates, we further develop a novel
              copula-based postprocessing approach of posterior predictive
              samples in order to mitigate the effect of the conditional
              independence assumption at the data level, and we demonstrate that
              our approach indeed provides accurate results.},
  number = {2},
  urldate = {2025-01-16},
  journal = {The Annals of Applied Statistics},
  author = {Jóhannesson, Árni V. and Siegert, Stefan and Huser, Raphaël and
            Bakka, Haakon and Hrafnkelsson, Birgir},
  month = jun,
  year = {2022},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {Approximate Bayesian inference, flood frequency analysis, latent
              Gaussian model, Max-and-Smooth, multivariate link function,
              spatiotemporal extremes},
  pages = {905--935},
  file = {Full Text PDF:/Users/karih/Zotero/storage/WZIBINW2/Jóhannesson et al.
          - 2022 - Approximate Bayesian inference for analysis of spatiotemporal
          flood frequency data.pdf:application/pdf},
}

@article{dyrrdal_bayesian_2015,
  title = {Bayesian hierarchical modeling of extreme hourly precipitation in {
           Norway}},
  volume = {26},
  copyright = {Copyright © 2014 John Wiley \& Sons, Ltd.},
  issn = {1099-095X},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2301},
  doi = {10.1002/env.2301},
  abstract = {Spatial maps of extreme precipitation are a critical component of
              flood estimation in hydrological modeling, as well as in the
              planning and design of important infrastructure. This is
              particularly relevant in countries, such as Norway, that have a
              high density of hydrological power generating facilities and are
              exposed to significant risk of infrastructure damage due to
              flooding. In this work, we estimate a spatially coherent map of the
              distribution of extreme hourly precipitation in Norway, in terms of
              return levels, by linking generalized extreme value (GEV)
              distributions with latent Gaussian fields in a Bayesian
              hierarchical model. Generalized linear models on the parameters of
              the GEV distribution are able to incorporate location-specific
              geographic and meteorological information and thereby accommodate
              these effects on extreme precipitation. Our model incorporates a
              Bayesian model averaging component that directly assesses model
              uncertainty in the effect of the proposed covariates. Gaussian
              fields on the GEV parameters capture additional unexplained spatial
              heterogeneity and overcome the sparse grid on which observations
              are collected. Our framework is able to appropriately characterize
              both the spatial variability of the distribution of extreme hourly
              precipitation in Norway and the associated uncertainty in these
              estimates. Copyright © 2014 John Wiley \& Sons, Ltd.},
  number = {2},
  urldate = {2025-01-16},
  journal = {Environmetrics},
  author = {Dyrrdal, Anita Verpe and Lenkoski, Alex and Thorarinsdottir, Thordis
            L. and Stordal, Frode},
  year = {2015},
  keywords = {generalized extreme value distributions, latent Gaussian processes
              , Markov chain Monte Carlo, return levels, short-term extreme
              precipitation, uncertainty assessment},
  pages = {89--106},
  file = {
          Snapshot:/Users/karih/Zotero/storage/G3QWMIVH/env.html:text/html;Submitted
          Version:/Users/karih/Zotero/storage/9S2U3BA4/Dyrrdal et al. - 2015 -
          Bayesian hierarchical modeling of extreme hourly precipitation in
          Norway.pdf:application/pdf},
}

@article{huerta_time-varying_2007,
  title = {Time-varying models for extreme values},
  volume = {14},
  issn = {1573-3009},
  url = {https://doi.org/10.1007/s10651-007-0014-3},
  doi = {10.1007/s10651-007-0014-3},
  abstract = {We propose a new approach for modeling extreme values that are
              measured in time and space. First we assume that the observations
              follow a Generalized Extreme Value (GEV) distribution for which the
              location, scale or shape parameters define the space–time
              structure. The temporal component is defined through a Dynamic
              Linear Model (DLM) or state space representation that allows to
              estimate the trend or seasonality of the data in time. The spatial
              element is imposed through the evolution matrix of the DLM where we
              adopt a process convolution form. We show how to produce temporal
              and spatial estimates of our model via customized Markov Chain
              Monte Carlo (MCMC) simulation. We illustrate our methodology with
              extreme values of ozone levels produced daily in the metropolitan
              area of Mexico City and with rainfall extremes measured at the
              Caribbean coast of Venezuela.},
  number = {3},
  urldate = {2025-01-16},
  journal = {Environmental and Ecological Statistics},
  author = {Huerta, Gabriel and Sansó, Bruno},
  month = sep,
  year = {2007},
  keywords = {Extreme values, GEV distribution, MCMC, Ozone levels, Process
              convolutions, Spatio-temporal process},
  pages = {285--299},
  file = {Full Text PDF:/Users/karih/Zotero/storage/UIARHGYC/Huerta and Sansó -
          2007 - Time-varying models for extreme values.pdf:application/pdf},
}

@article{thorarinsdottir_bayesian_2018,
  title = {Bayesian {Regional} {Flood} {Frequency} {Analysis} for {Large} {
           Catchments}},
  volume = {54},
  copyright = {©2018. American Geophysical Union. All Rights Reserved.},
  issn = {1944-7973},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2017WR022460},
  doi = {10.1029/2017WR022460},
  abstract = {Regional flood frequency analysis is commonly applied in
              situations where there exists insufficient data at a location for a
              reliable estimation of flood quantiles. We develop a Bayesian
              hierarchical modeling framework for a regional analysis of data
              from 203 large catchments in Norway with the generalized extreme
              value distribution as the underlying model. Generalized linear
              models on the parameters of the generalized extreme value
              distribution are able to incorporate location-specific geographic
              and meteorological information and thereby accommodate these
              effects on the flood quantiles. A Bayesian model averaging
              component additionally assesses model uncertainty in the effect of
              the proposed covariates. The resulting regional model is seen to
              give substantially better predictive performance than the regional
              model currently used in Norway.},
  number = {9},
  urldate = {2025-01-16},
  journal = {Water Resources Research},
  author = {Thorarinsdottir, Thordis L. and Hellton, Kristoffer H. and Steinbakk
            , Gunnhildur H. and Schlichting, Lena and Engeland, Kolbjørn},
  year = {2018},
  keywords = {flood frequency analysis, Bayesian},
  pages = {6929--6947},
  file = {Full Text PDF:/Users/karih/Zotero/storage/HNZVJVAA/Thorarinsdottir et
          al. - 2018 - Bayesian Regional Flood Frequency Analysis for Large
          Catchments.pdf:application/pdf;Snapshot:/Users/karih/Zotero/storage/NVFCH3FF/2017WR022460.html:text/html
          },
}

@incollection{hazra_bayesian_2023,
  address = {Cham},
  title = {Bayesian {Latent} {Gaussian} {Models} for {High}-{Dimensional} {
           Spatial} {Extremes}},
  isbn = {978-3-031-39791-2},
  url = {https://doi.org/10.1007/978-3-031-39791-2_7},
  abstract = {In this chapter, we show how to efficiently model high-dimensional
              extreme peaks-over-threshold events over space in complex
              non-stationary settings, using extended latent Gaussian models
              (LGMs), and how to exploit the fitted model in practice for the
              computation of long-term return levels. The extended LGM framework
              assumes that the data follow a specific parametric distribution,
              whose unknown parameters are transformed using a multivariate link
              function and are then further modeled at the latent level in terms
              of fixed and random effects that have a joint Gaussian
              distribution. In the extremal context, we here assume that the
              response level distribution is described in terms of a Poisson
              point process likelihood, motivated by asymptotic extreme-value
              theory, and which conveniently exploits information from all
              threshold exceedances. This contrasts with the more common
              data-wasteful approach based on block maxima, which are typically
              modeled with the generalized extreme-value (GEV) distribution. When
              conditional independence can be assumed at the response level and
              latent random effects have a sparse probabilistic structure, fast
              approximate Bayesian inference becomes possible in very high
              dimensions, and we here present the recently proposed inference
              approach called “Max-and-Smooth,” which provides exceptional
              speedup compared to alternative methods. The proposed methodology
              is illustrated by application to satellite-derived precipitation
              data over Saudi Arabia, obtained from the Tropical Rainfall
              Measuring Mission, with 2738 grid cells and about 20 million
              spatio-temporal observations in total. Our fitted model captures
              the spatial variability of extreme precipitation satisfactorily,
              and our results show that the most intense precipitation events are
              expected near the southwestern part of Saudi Arabia, along the Red
              Sea coastline.},
  urldate = {2025-01-16},
  booktitle = {Statistical {Modeling} {Using} {Bayesian} {Latent} {Gaussian} {
               Models} : {With} {Applications} in {Geophysics} and {Environmental
               } {Sciences}},
  publisher = {Springer International Publishing},
  author = {Hazra, Arnab and Huser, Raphaël and Jóhannesson, Árni V.},
  editor = {Hrafnkelsson, Birgir},
  year = {2023},
  doi = {10.1007/978-3-031-39791-2_7},
  pages = {219--251},
}

@article{carpenter_stan_2017,
  title = {Stan: {A} {Probabilistic} {Programming} {Language}},
  volume = {76},
  copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D.
               Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus
               Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
  issn = {1548-7660},
  shorttitle = {Stan},
  url = {https://doi.org/10.18637/jss.v076.i01},
  doi = {10.18637/jss.v076.i01},
  abstract = {Stan is a probabilistic programming language for specifying
              statistical models. A Stan program imperatively defines a log
              probability function over parameters conditioned on specified data
              and constants. As of version 2.14.0, Stan provides full Bayesian
              inference for continuous-variable models through Markov chain Monte
              Carlo methods such as the No-U-Turn sampler, an adaptive form of
              Hamiltonian Monte Carlo sampling. Penalized maximum likelihood
              estimates are calculated using optimization methods such as the
              limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is
              also a platform for computing log densities and their gradients and
              Hessians, which can be used in alternative algorithms such as
              variational Bayes, expectation propagation, and marginal inference
              using approximate integration. To this end, Stan is set up so that
              the densities, gradients, and Hessians, along with intermediate
              quantities of the algorithm such as acceptance probabilities, are
              easily accessible. Stan can be called from the command line using
              the cmdstan package, through R using the rstan package, and through
              Python using the pystan package. All three interfaces support
              sampling and optimization-based inference with diagnostics and
              posterior analysis. rstan and pystan also provide access to log
              probabilities, gradients, Hessians, parameter transforms, and
              specialized plotting.},
  urldate = {2025-01-16},
  journal = {Journal of Statistical Software},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee,
            Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus
            and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  month = jan,
  year = {2017},
  keywords = {algorithmic differentiation, Bayesian inference, probabilistic
              programming, Stan},
  pages = {1--32},
  file = {Full Text:/Users/karih/Zotero/storage/XY4EF66Z/Carpenter et al. - 2017
          - Stan A Probabilistic Programming Language.pdf:application/pdf},
}

@book{r_core_team_r_2024,
  address = {Vienna, Austria},
  title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
  url = {https://www.R-project.org/},
  publisher = {R Foundation for Statistical Computing},
  author = {{R Core Team}},
  year = {2024},
}

@misc{stan_development_team_convergence_nodate,
  title = {Convergence and efficiency diagnostics for {Markov} {Chains} — {Rhat}
           },
  url = {https://mc-stan.org/rstan/reference/Rhat.html},
  abstract = {These functions are improved versions of the traditional Rhat (for
              convergence) and Effective Sample Size (for efficiency).},
  urldate = {2025-01-16},
  author = {Stan Development Team},
  file = {Snapshot:/Users/karih/Zotero/storage/VVT2VEHW/Rhat.html:text/html},
}

@inproceedings{watanabe_higher_2018,
  address = {Cham},
  title = {Higher {Order} {Equivalence} of {Bayes} {Cross} {Validation} and {
           WAIC}},
  isbn = {978-3-319-97798-0},
  doi = {10.1007/978-3-319-97798-0_3},
  abstract = {It was proved in the previous paper (Watanabe, J Mach Learn Res,
              11:3571–3591, (2010), [16]) that Bayes cross validation is
              asymptotically equivalent to the widely applicable information
              criterion (WAIC), even if the posterior distribution can not be
              approximated by any normal distribution. In the present paper, we
              prove that they are equivalent to each other according to the
              second order asymptotics, if the posterior distribution can be
              approximated by some normal distribution. Based on this equivalence
              , it is also shown that the Bayes cross validation and WAIC are
              asymptotically equivalent as functions of a hyperparameter of a
              prior.},
  booktitle = {Information {Geometry} and {Its} {Applications}},
  publisher = {Springer International Publishing},
  author = {Watanabe, Sumio},
  editor = {Ay, Nihat and Gibilisco, Paolo and Matúš, František},
  year = {2018},
  pages = {47--73},
}

@article{vehtari_practical_2017,
  title = {Practical {Bayesian} model evaluation using leave-one-out
           cross-validation and {WAIC}},
  volume = {27},
  issn = {1573-1375},
  url = {https://doi.org/10.1007/s11222-016-9696-4},
  doi = {10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable
              information criterion (WAIC) are methods for estimating pointwise
              out-of-sample prediction accuracy from a fitted Bayesian model
              using the log-likelihood evaluated at the posterior simulations of
              the parameter values. LOO and WAIC have various advantages over
              simpler estimates of predictive error such as AIC and DIC but are
              less used in practice because they involve additional computational
              steps. Here we lay out fast and stable computations for LOO and
              WAIC that can be performed using existing simulation draws. We
              introduce an efficient computation of LOO using Pareto-smoothed
              importance sampling (PSIS), a new procedure for regularizing
              importance weights. Although WAIC is asymptotically equal to LOO,
              we demonstrate that PSIS-LOO is more robust in the finite case with
              weak priors or influential observations. As a byproduct of our
              calculations, we also obtain approximate standard errors for
              estimated predictive errors and for comparison of predictive errors
              between two models. We implement the computations in an R package
              called loo and demonstrate using models fit with the Bayesian
              inference package Stan.},
  number = {5},
  urldate = {2025-01-17},
  journal = {Statistics and Computing},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  month = sep,
  year = {2017},
  keywords = {Artificial Intelligence, Bayesian computation, K-fold
              cross-validation, Leave-one-out cross-validation (LOO), Pareto
              smoothed importance sampling (PSIS), Stan, Widely applicable
              information criterion (WAIC)},
  pages = {1413--1432},
  file = {Full Text PDF:/Users/karih/Zotero/storage/6CEWLHP3/Vehtari et al. -
          2017 - Practical Bayesian model evaluation using leave-one-out
          cross-validation and WAIC.pdf:application/pdf},
}
